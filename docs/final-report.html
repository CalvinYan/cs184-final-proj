
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <style>
    body {
      background-color: #404040;
      background-color: white;
      padding: 100px;
      width: 1000px;
      margin: auto;
      text-align: left;
      font-weight: 300;
      font-family: 'Open Sans', sans-serif;
      color: #121212;
    }
    h1, h2, h3, h4 {
      font-family: 'Source Sans Pro', sans-serif;
    }
    kbd {
      color: #121212;
    }
    /* blockquote {
      color: #888;
      border: 2px solid #333;
      padding: 10px;
      background-color: #ccc;
    } */

    table.custom-tbl {
      border: 1px solid;
    }

    table.custom-tbl th {
      border: 1px solid;
      background-color: rgb(99, 209, 209);
    }

    table.custom-tbl td {
      border: 1px solid;
      background-color: #f1e686a8;
    }

    /* The alert message box */
    .alert {
      padding: 20px;
      background-color: #f44336; /* Red */
      color: white;
      margin-bottom: 15px;
    }

    /* The close button */
    .closebtn {
      margin-left: 15px;
      color: white;
      font-weight: bold;
      float: right;
      font-size: 22px;
      line-height: 20px;
      cursor: pointer;
      transition: 0.3s;
    }

    /* When moving the mouse over the close button */
    .closebtn:hover {
      color: black;
    }
  </style>

  <title>CS 184 Augmented Reality Final Project Milestone</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <!-- Not using below due to lacking bold fontfaces -->
  <!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro|Source+Sans+Pro:400,700" rel="stylesheet"> -->
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono|Roboto+Slab|Roboto:300,400,500,700" rel="stylesheet" />

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>  
  <script async="" src="https://embed.reddit.com/widgets.js" charset="UTF-8"></script>
</head>

<body
  style="padding-top: 50px;">

  <div align="middle">
    <img src="./images/head.png" 
        align="middle" 
        width="1000vw" />
  </div>
  
  <h1 align="left">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
  <h1 align="left">Final Project: Photorealistic Rendering in Augmented Reality</h1>
  <h2 align="left">Yuntian Ke, Calvin Yan, Ruichi Zhang, Wentao Zhang</h2>

  <h2>Abstract</h2>
  <p>In this project we have implemented photorealistic rendering in Augmented Reality based on image-based lighting. We use ArUco 
    (binary square fiducial marker) as the marker and combine OpenCV and OpenGL to detect the marker and render AR object on it. 
    We have also implemented texture mapping. Then, we extracted the light probes in our scene to create a basic light-based model
    and finally rendered some photorealistic AR objects.
  </p>

  <h2>Technical Approach</h2>
  <p> At this stage our project has accomplished four things:</p>
  <ol>
    <li> It detects fiducial markers in a live video feed. </li>
    <li> It estimates the pose of the camera relative to the marker in the scene. </li>
    <li> It uses the estimated pose to render a scene over the marker in a realistic perspective </li>
    <li> It employs image-based lighting techniques to adapt the render to its lighting environment, inducing
          photorealism
    </li>
  </ol>

  <h3> Marker detection </h3>
  <p>
    We will explain how the marker detection part is decided and implemented. <br>
    At the beginning, we search a tutorial online, which uses the ORB(Oriented Rotated BRIEF) method in OpenCV, 
    which is able to get the features of the given image by smoothing the image and then recognize the input video, 
    if there is something shares the same extracted features, then we recognize that object in the given video(or image) 
    as the target for our future rendering part. However, we do not need to ensure such a generality because arbitrary image
    brings higher requirement for the quality of the video and we have to make the target object clear enough in the environment,
    not to mention the target marker must be as flat as possible.<br>
    As shown in the vedio below, the implemented code using ORB is not ideal at all, we can see that there are non-ignorable 
    noise on the detection, so the result of the rendered model on the target is pretty unstable.<br>
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <video width="400" height="300" controls autoplay>
              <source src="videos/orb.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
      </table>
    </div>

    Then, we elected to use Aruco marker to do location detection. This process is based on finding correspondences between points in the real environment and their 2D image projection.
  </p>
  <p>some example of Aruco marker</p>

  <div align="middle">
    <table style="width=100%">
      <tr>
        <td>
          <img src="images/marker.png" align="middle" width="300px"/>
          <figcaption align="middle">ArUco marker</figcaption>
        </td>
      </tr>
    </table>
  </div>

  <p>
    An ArUco marker is a square marker with a black border and a binary matrix inside that identifies it. 
    The black border helps it to be quickly detected in an image and the binary code allows for identification and error correction techniques.
  </p>
  <p>it can detect the rotation of the marker based on binary codification</p>
  <p>two steps to do marker detection 
    1.The detection of marker candidates involves analyzing an image to find square shapes that could be markers. 
    This is done through adaptive thresholding and extracting contours from the thresholded image, 
    then filtering out those that are not convex or do not approximate to a square shape.
    2.analyze their inner codification to determine if they are actually markers. 
    This involves perspective transformation to obtain the marker in its canonical form, thresholding to separate white and black bits, and counting the amount of black or white pixels in each cell to determine if it is a white or black bit. 
    The bits are then analyzed to see if they belong to a specific dictionary, and error correction techniques are used if necessary.
  </p>

  <h3> Pose estimation </h3>
  <p>Then, we need to obtain camera pose. First, do calibrateCamera(), so that we can get the camera matrix and the distortion coefficinets.Then we can draw the axis(we show video for this in result part). 
    This can be done by cv::aruco::drawAxis</p>
  <p>Last but not least, we have to caliborate the camera by ourselves first. </p>

  <h3> Rendering </h3>
  <p>
    Since we will use OpenGL for the rendering of our AR object, we need to link the points we got from last step to our new scene created by OpenGL.
    First, we convert the original image captured by our camera to OpenGL texture format and draw the scene based on this converted image.
    Then, we load our object and overlay it on our scene based on the view matrix we defined.
  </p>
    

  <h3> Image-based lighting </h3>
    <p>First, we go through code in Project4 to find some useful function for IBL. In project4, it used Cubemap to do IBL. However, we found that in Cubemap, we must have 360 degree view(six sides for Cubemap), which can not be used in our final project, for we do not have 360 degree view. Moreover, it can be hard to implement in real-time situation.
      So then we look up through other methods that can satisfy our goals.
    </p>
  
  
  
  
  
  
  
  
  
  <h3>Reflections</h3>
  <p>
    Through our implementation of our project, we had several key takeaways:
    <ol>
      <li> Set realistic goals. At the start we allowed curiosity and excitement to lead our design process. In the end, however, much more time was spent on tedious work than implementing exciting, cutting-edge techniques in computer graphics. Much of the project was spent overcoming technical roadblocks like setting up the OpenGL rendering pipeline from scratch and integrating it with OpenCV. We severely overestimated the degree to which past projects prepared us for this (in fact, we ended up not reusing project
      code at all), and it was perhaps the most difficult and painstaking part of project development. Initial goals like use of animation and light probes were demonstrably out of reach, but we failed to consider this.
      <li> This stuff is hard! 
    </ol>
    
  </p>
  </br>
  <p>
    Despite these challenges, we're immensly proud of the work that we've done, and hope that it has adequately
    demonstrated what we have learned in this course, from mesh geometry to texture mapping to ray tracing to camera anatomy. We were unable to implement some of the features we were most excited about, but with more time and
    resources, we would surely love to explore them further:
    <ul>
      <ol
    </ul>
  </p>
  
  <h2>Results</h2>

  <p> This part, we apply Aruco marker, which is showed on the below video. we make the marker detection efficient enough for the live demo and stable enough to render some models on it</p>
  <div align="middle">
    <table style="width=100%">
      <tr>
        <td>
          <video width="400" height="300" controls autoplay>
            <source src="videos/Aruco_marker.mp4" type="video/mp4">
            <object data="ks-50000.mp4" width="500" height="240">
                <embed width="420" height="300" src="Aruco_marker.mp4">
            </object>
          </video>
        </td>
      </tr>
    </table>
  </div>
  <p>The below video is about our main model's demo. You can clearly see that, it can be stable, when we move the ArUco marker. 
    Though, sometimes, at some specific location, it can be a liitle bit unstable.</p>
  <div align="middle">
    <table style="width=100%">
      <tr>
        <td>
          <video width="400" height="300" controls autoplay>
            <source src="videos/ar_demo.mp4" type="video/mp4">
            <object data="ks-50000.mp4" width="500" height="240">
                <embed width="420" height="300" src="ar_demo.mp4">
            </object>
          </video>
        </td>
      </tr>
    </table>
  </div>

  <h3> Image Based Lighting </h3>
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="images/dark.png" align="middle" width="300px"/>
            <figcaption align="middle">dark environment</figcaption>
          </td>
          <td>
            <img src="images/bright.png" align="middle" width="300px"/>
            <figcaption align="middle">bright environment</figcaption>
          </td>
        </tr>
      </table>
    </div>
  
  <p>this is the part where we implement the Image Based Lighting. The left one is at a dark environment. The right one is at a bright environment.</p>

  <p>Additionally, we do some texture mapping on this teapot in below video</p>
  <div align="middle">
    <table style="width=100%">
      <tr>
        <td>
          <video width="400" height="300" controls autoplay>
            <source src="videos/zrckyt.mp4" type="video/mp4">
            <object data="ks-50000.mp4" width="500" height="240">
                <embed width="420" height="300" src="zrckyt.mp4">
            </object>
          </video>
        </td>
      </tr>
    </table>
  </div>
  <p>it might be a little bit unstable, but you can see from video, it has quite good effects.</p>
    
    <h2>Contributions from each group member</h2> 
    <p>Calvin Yan: Decided on the project topic and scope, developed final marker tracking, rendering, and image based lighting code

    </p>

    <p>Ruichi Zhang : Did some early researches and found staring point, implemented texture mapping on AR project, helped with development of some code
      
    </p>

    <p>Yuntian Ke: Go through papers to find good IBL which can let us use it in real-time and even in situations we don't have 360 degree view. Prepare for slices; implement marker tracking for some simple models from start. 
      
    </p>

    <p>Wentao Zhang :Prepare for the slides; Search for the image based lighting algorithm used in the final presentation; Finding the starter code of detection and try to implement it(both milestone and the final one)
      
    </p>

  <h2>References</h2>
  <!-- List what resources, e.g. books, papers and/or online resources that are references for your project. List the computing platform, hardware and software resources that you will use for your project. You have a wide latitude here to use what you have access to, but be aware that you will have to support and trouble-shoot on your platform yourselves. If you are starting from an existing piece of code or system, describe and provide a pointer to it here.
    -->
    <p> Here are our final presentations:
      <ul>
        <li> <a href=https://docs.google.com/presentation/d/1z33PdWrq0jGQSRuR-vE-wGZcMeVtY2xqALYma_eQw54> For professors </a>
        <li> <a href="com/presentation/d/1gwT1bwrUjPi4Uptc22YvsFdWho37op7oM9qZ996DNME"> For TAs </a>
      </ul>
    </p>
    <p>Our milestone prototype was based on the following tutorials:
      <ul>
        <li> <a href="https://pyimagesearch.com/2021/01/04/opencv-augmented-reality-ar/">2D Augmented Reality with OpenCV using ArUco </a> </li>
        <li> <a href="https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/">3D Augmented Reality with Projective Geometry</a></p> </li>
      </ul>
      However, we want to emphasize that our final implementation ultimately strayed from these resources, and consists of original work.
  <p>
    As before, we have used the work of Ian Curtis as a source of aesthetic inspiration for our final product:  <a href="https://twitter.com/XRarchitect">Twitter</a> and <a href="https://www.instagram.com/ianjcurtis/">Instagram</a>
  </p>
  <p>
    Below are some resources we explored and discussed about for image-based lighting:
    <ul>
      <li>
        <a href="https://arxiv.org/pdf/2301.05211.pdf">Accidental Light Probes</a>
      </li>
      <li>
        <a href="https://history.siggraph.org/wp-content/uploads/2022/10/2016-Poster-64-Baba_Estimating-Lighting-Environments-Based-on-Shadow-Area-in-an-Omni-Directional-Image.pdf">Estimating Lighting Environments Based on Shadow Area in an Omni-Directional Image</a>
      </li>
      <li>
        <a href="https://developer.nvidia.cn/gpugems/gpugems/part-iii-materials/chapter-19-image-based-lighting">
          Converting images to cubemaps for local reflection
        </a>
      </li>
      <li>
        <a href="https://www.trentreed.net/blog/physically-based-shading-and-image-based-lighting/">
          Physically based Shading and image based lighting
        </a>
      </li>
      <li>
        <a href="https://github.com/sourabhkhemka/Thirsty-Crow-E-yantra/blob/master/GLteapot.py">
          Basic texture mappting implementation
        </a>
      </li>
      <li>
        <a href="https://www.pauldebevec.com/Research/IBL/debevec-siggraph98.pdf">
          Implementation for image based lighting
        </a>
      </li>
      
      
    </ul>
  </p>

  <p>
    As well as other resources that were immensely helpful for development:
    <ul>
      <li>
        <a href="https://docs.opencv.org/4.x/index.html">OpenCV documentation</a> for video processing, fiducial marker tracking and positioning
      </li>
      <li>
        The <a href="https://www.cs.utexas.edu/users/fussell/courses/cs354/handouts/Addison.Wesley.OpenGL.Programming.Guide.8th.Edition.Mar.2013.ISBN.0321773039.pdf">Red Book</a>: a comprehensive guide to learning OpenGL
      </li>

    </ul>
  </p>

</body>
</html>