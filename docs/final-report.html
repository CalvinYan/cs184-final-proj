
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <style>
    body {
      background-color: #404040;
      background-color: white;
      padding: 100px;
      width: 1000px;
      margin: auto;
      text-align: left;
      font-weight: 300;
      font-family: 'Open Sans', sans-serif;
      color: #121212;
    }
    h1, h2, h3, h4 {
      font-family: 'Source Sans Pro', sans-serif;
    }
    kbd {
      color: #121212;
    }
    /* blockquote {
      color: #888;
      border: 2px solid #333;
      padding: 10px;
      background-color: #ccc;
    } */

    table.custom-tbl {
      border: 1px solid;
    }

    table.custom-tbl th {
      border: 1px solid;
      background-color: rgb(99, 209, 209);
    }

    table.custom-tbl td {
      border: 1px solid;
      background-color: #f1e686a8;
    }

    /* The alert message box */
    .alert {
      padding: 20px;
      background-color: #f44336; /* Red */
      color: white;
      margin-bottom: 15px;
    }

    /* The close button */
    .closebtn {
      margin-left: 15px;
      color: white;
      font-weight: bold;
      float: right;
      font-size: 22px;
      line-height: 20px;
      cursor: pointer;
      transition: 0.3s;
    }

    /* When moving the mouse over the close button */
    .closebtn:hover {
      color: black;
    }
  </style>

  <title>CS 184 Augmented Reality Final Project Milestone</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <!-- Not using below due to lacking bold fontfaces -->
  <!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro|Source+Sans+Pro:400,700" rel="stylesheet"> -->
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono|Roboto+Slab|Roboto:300,400,500,700" rel="stylesheet" />

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>  
  <script async="" src="https://embed.reddit.com/widgets.js" charset="UTF-8"></script>
</head>

<body
  style="padding-top: 50px;">

  <div align="middle">
    <img src="./images/head.png" 
        align="middle" 
        width="1000vw" />
  </div>
  
  <h1 align="left">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
  <h1 align="left">Final Project: Photorealistic Rendering in Augmented Reality</h1>
  <h2 align="left">Yuntian Ke, Calvin Yan, Ruichi Zhang, Wentao Zhang</h2>

  <h2>Abstract</h2>
  <p>In this project we have implemented photorealistic rendering in Augmented Reality based on image-based lighting. We use ArUco 
    (binary square fiducial marker) as the marker and combine OpenCV and OpenGL to detect the marker and render AR object on it. 
    We have also implemented texture mapping. Then, we extracted the light probes in our scene to create a basic light-based model
    and finally rendered some photorealistic AR objects.
  </p>

  <h2>Technical Approach</h2>
  <p> At this stage our project has accomplished three things:</p>
  <ol>
    <li> It detects fiducial markers in a live video feed. </li>
    <li> It estimates the pose of the camera relative to the marker in the scene. </li>
    <li> It uses the estimated pose to render a scene over the marker in a realistic perspective </li>
  </ol>

  <p>
    We will explain how the marker detection part is decided and implemented. <br>
    At the beginning, we search a tutorial online, which uses the ORB(Oriented Rotated BRIEF) method in OpenCV, 
    which is able to get the features of the given image by smoothing the image and then recognize the input video, 
    if there is something shares the same extracted features, then we recognize that object in the given video(or image) 
    as the target for our future rendering part. However, we do not need to ensure such a generality because arbitrary image
    brings higher requirement for the quality of the video and we have to make the target object clear enough in the environment,
    not to mention the target marker must be as flat as possible.<br>
    As shown in the vedio below, the implemented code using ORB is not ideal at all, we can see that there are non-ignorable 
    noise on the 
    

    First, we elected to use Aruco marker to do location detection. This process is based on finding correspondences between points in the real environment and their 2d image projection.
  </p>
  <p>some example of Aruco marker</p>

  <div align="middle">
    <table style="width=100%">
      <tr>
        <td>
          <img src="images/marker.png" align="middle" width="300px"/>
          <figcaption align="middle">ArUco markere</figcaption>
        </td>
      </tr>
    </table>
  </div>

  <p>
    An ArUco marker is a square marker with a black border and a binary matrix inside that identifies it. 
    The black border helps it to be quickly detected in an image and the binary code allows for identification and error correction techniques.
  </p>
  <p>it can detect the rotation of the marker based on binary codification</p>
  <p>two steps to do marker detection 
    1.The detection of marker candidates involves analyzing an image to find square shapes that could be markers. 
    This is done through adaptive thresholding and extracting contours from the thresholded image, 
    then filtering out those that are not convex or do not approximate to a square shape.
    2.analyze their inner codification to determine if they are actually markers. 
    This involves perspective transformation to obtain the marker in its canonical form, thresholding to separate white and black bits, and counting the amount of black or white pixels in each cell to determine if it is a white or black bit. 
    The bits are then analyzed to see if they belong to a specific dictionary, and error correction techniques are used if necessary.
  </p>
  <p>then, we need to obtain camera pose. First, do calibrateCamera(), so that we can get the camera matrix and the distortion coefficinets.Then we should draw the  this can be done by cv::aruco::drawAxis</p>


    <p>For the next step, we also want to build 3D models on the reference plane. This is equivalent to adding the column for z axis in our
    original transformation matrix. We could use the two column that we already got to estimate the third column for the z axis. Then, we are able
    to render 3D object in our camera.</p>
    
  <h3>Reflection</h3>
  <p>

  <h2>Results</h2>

  <p> This part, we apply Aruco marker, which is showed on the below video. we make the marker detection efficient enough for the live demo and stable enough to render some models on it</p>
  <div align="middle">
    <table style="width=100%">
      <tr>
        <td>
          <video width="400" height="300" controls autoplay>
            <source src="videos/Aruco_marker.mp4" type="video/mp4">
            <object data="ks-50000.mp4" width="500" height="240">
                <embed width="420" height="300" src="Aruco_marker.mp4">
            </object>
          </video>
        </td>
      </tr>
    </table>
  </div>

  <div align="middle">
    <table style="width=100%">
      <tr>
        <td>
          <video width="400" height="300" controls autoplay>
            <source src="videos/ar_demo.mp4" type="video/mp4">
            <object data="ks-50000.mp4" width="500" height="240">
                <embed width="420" height="300" src="ar_demo.mp4">
            </object>
          </video>
        </td>
      </tr>
    </table>
  </div>

  <h3> Image Based Lighting </h3>
    <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="images/dark.png" align="middle" width="300px"/>
            <figcaption align="middle">dark environment</figcaption>
          </td>
          <td>
            <img src="images/bright.png" align="middle" width="300px"/>
            <figcaption align="middle">bright environment</figcaption>
          </td>
        </tr>
      </table>
    </div>
  
  <p>this is the part where we implement the Image Based Lighting. The left one is at a dark environment. The right one is at a bright environment.</p>

  <p>Additionally, we do some texture mapping on this teapot in below video</p>
  <div align="middle">
    <table style="width=100%">
      <tr>
        <td>
          <video width="400" height="300" controls autoplay>
            <source src="videos/zrckyt.mp4" type="video/mp4">
            <object data="ks-50000.mp4" width="500" height="240">
                <embed width="420" height="300" src="zrckyt.mp4">
            </object>
          </video>
        </td>
      </tr>
    </table>
  </div>
  <p>it might be a little bit unstable, but you can see from video, it has quite good effects.</p>
    
    <h2>Contributions from each group member</h2>
    <p>Calvin Yan: Decided on the project topic and scope, developed final marker tracking, rendering, and image based lighting code

    </p>

    <p>Ruichi Zhang : Did some early researches and find staring point, implemented texture mapping, 
      
    </p>

    <p>Yuntian Ke: Go through papers to find good IBL which can let us use it in real-time and even in situations we don't have 360 degree view. Prepare for slices; try to implement marker tracking for some simple models at the beginning. 
      
    </p>

    <p>Wentao Zhang :Prepare for the slides; Search for the image based lighting algorithm used in the final presentation; Finding the starter code of detection and try to implement it(both milestone and the final one)
      
    </p>

  <h2>References</h2>
  <!-- List what resources, e.g. books, papers and/or online resources that are references for your project. List the computing platform, hardware and software resources that you will use for your project. You have a wide latitude here to use what you have access to, but be aware that you will have to support and trouble-shoot on your platform yourselves. If you are starting from an existing piece of code or system, describe and provide a pointer to it here.
    -->
    <p>Most of our work so far started from these two tutorials: 
      <ul>
        <li> <a href="https://pyimagesearch.com/2021/01/04/opencv-augmented-reality-ar/">2D Augmented Reality with OpenCV using ArUco </a> </li>
        <li> <a href="https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/">3D Augmented Reality with Projective Geometry</a></p> </li>
      </ul>
  <p>
    Additionally, as before we will use the work of Ian Curtis as a source of aesthetic inspiration for our final product:  <a href="https://twitter.com/XRarchitect">Twitter</a> and <a href="https://www.instagram.com/ianjcurtis/">Instagram</a>
  </p>
  <p>
    Below are some resources we explored and discussed about for image-based lighting:
    <ul>
      <li>
        <a href="https://arxiv.org/pdf/2301.05211.pdf">Accidental Light Probes</a>
      </li>
      <li>
        <a href="https://history.siggraph.org/wp-content/uploads/2022/10/2016-Poster-64-Baba_Estimating-Lighting-Environments-Based-on-Shadow-Area-in-an-Omni-Directional-Image.pdf">Estimating Lighting Environments Based on Shadow Area in an Omni-Directional Image</a>
      </li>
      <li>
        <a href="https://developer.nvidia.cn/gpugems/gpugems/part-iii-materials/chapter-19-image-based-lighting">
          Converting images to cubemaps for local reflection
        </a>
      </li>
      <li>
        <a href="https://www.trentreed.net/blog/physically-based-shading-and-image-based-lighting/">
          Physically based Shading and image based lighting
        </a>
      </li>
      <li>
        <a href="https://github.com/sourabhkhemka/Thirsty-Crow-E-yantra/blob/master/GLteapot.py">
          Basic texture mappting implementation
        </a>
      </li>
      <li>
        <a href="https://www.pauldebevec.com/Research/IBL/debevec-siggraph98.pdf">
          Implementation for image based lighting
        </a>
      </li>
      
      
    </ul>
  </p>

  

  <p>
    As well as other development resources:
    <ul>
      <li>
        <a href="https://developers.google.com/ar/develop">Google ARCore</a> for mobile development and augmented AR features like environmental lighting estimation
      </li>
      <li>
        <a href="https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html">OpenCV tutorial</a> for fiducial marker tracking and positioning
      </li>
    </ul>
  </p>

</body>
</html>